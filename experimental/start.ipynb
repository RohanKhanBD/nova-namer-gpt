{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as Optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62262\n",
      "['Rieder Berg\\n', 'Alttiefenweg\\n', 'Goßmannsdorf\\n', 'Gemeindebühl\\n', 'Mader\\n', 'Kroissenhof\\n', 'Schlappenreuth\\n', 'Obermitterdorf\\n', 'Ullading\\n', 'Großköllnbach\\n']\n"
     ]
    }
   ],
   "source": [
    "# import text & shuffle set\n",
    "with open(\"./assets/names.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    names = file.readlines()\n",
    "# shuffle it\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "# dataset size\n",
    "print(len(names))\n",
    "print(names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ä', 'Ö', 'Ü', 'ß', 'ä', 'ö', 'ü']\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "# setup vocabulary\n",
    "all_chars = list(sorted(set([(\"\".join(char)) for name in names for char in name])))\n",
    "print(all_chars)\n",
    "vocab_size = len(all_chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 8000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "# configured to port data to mac gpu if available\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "#weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary mapping dicts\n",
    "itos = {i:s for i, s in enumerate(all_chars)}\n",
    "stoi = {s:i for i, s in itos.items()}\n",
    "#print(itos)\n",
    "#print(stoi)\n",
    "# voc encoding / decoding functions\n",
    "encode = lambda input: [stoi[i] for i in input]\n",
    "decode = lambda input: \"\".join([itos[i] for i in input])\n",
    "#print(encode(names[0]))\n",
    "#print(decode(encode(names[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540104 67513 67513\n"
     ]
    }
   ],
   "source": [
    "# convert names list to data: concat text, encode it, tensor it\n",
    "data = torch.tensor(encode(\"\".join(names)), dtype=torch.long)\n",
    "# split data into train / dev / test with 0.8 / 0.1 / 0.1\n",
    "border_1 = int(0.8 * len(data))\n",
    "border_2 = int(0.9 * len(data))\n",
    "train_split = data[:border_1]\n",
    "dev_split = data[border_1:border_2]\n",
    "test_split = data[border_2:]\n",
    "print(len(train_split), len(dev_split), len(test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 64]) torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "# data loading: deliver batches of X, Y tensors for chosen split\n",
    "torch.manual_seed(42)\n",
    "def get_batch(split):\n",
    "    \"\"\" delivers a batch of X, Y tensors for specified split\"\"\"\n",
    "    # get random numbers (in amount of \"batch_size\") within split boundaries to grab data for the batch samples\n",
    "    batch_borders = torch.randint(0, len(split)-context_len, (batch_size,))\n",
    "    x = torch.stack([split[t : t+context_len] for t in batch_borders])\n",
    "    y = torch.stack([split[t+1 : t+context_len+1] for t in batch_borders])\n",
    "    return x, y\n",
    "x, y = get_batch(train_split)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate loss function outsite backprop; called from training function after defined training steps\n",
    "@torch.no_grad()\n",
    "def check_loss():\n",
    "    m.eval()\n",
    "    out = {}\n",
    "    # calc train & dev loss as averages after defined eval steps\n",
    "    for split in [train_split, dev_split]:\n",
    "        losses = torch.zeros(eval_iter)\n",
    "        # calc loss for every batch and save result into tensor\n",
    "        for i in range(eval_iter):\n",
    "            x, y = get_batch(split)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            _, loss = m(x, y)\n",
    "            losses[i] = loss.item()\n",
    "        out[split] = losses.mean() \n",
    "    m.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single self-attention head; called from multi-head-attention class\n",
    "class Head(nn.Module):\n",
    "    \n",
    "    def __init__(self, h_size):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(in_features=n_embd, out_features=h_size, bias=False)\n",
    "        self.key = nn.Linear(in_features=n_embd, out_features=h_size, bias=False)\n",
    "        self.value = nn.Linear(in_features=n_embd, out_features=h_size, bias=False)\n",
    "        # helper matrix for triangular masking; pre-registered as full-size buffer for performance; all zero values above the diagonal\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(context_len, context_len)))\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # B, T, H\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        # B, T, T\n",
    "        wei = q @ torch.transpose(k, dim0=-1, dim1=-2) * C**-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.drop(wei)\n",
    "        # B, T, H\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple heads of self-attention in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList( Head(head_size) for _ in range(n_head))\n",
    "        # linear projection layer to blend all cat head outputs\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # cat / stack each head's out_features along last dim to total of n_embd out_features\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.drop(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp layer with relu; widened first linear layer\n",
    "class Ffw(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * 4, bias=None),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_embd * 4, n_embd, bias=None),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer block: communication in multi-head-attention, then computation in ffw layers\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.multi_head_sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffw = Ffw()\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.multi_head_sa((self.ln1(x)))\n",
    "        x = x + self.ffw(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core GPT logic setting up NN\n",
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # embeddings\n",
    "        self.tok_embeddings = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embeddings = nn.Embedding(context_len, n_embd)\n",
    "        # transformer blocks of amount n_layer\n",
    "        self.t_blocks = nn.Sequential(*[TransformerBlock() for _ in range(n_layer)])\n",
    "        # output layer\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    " \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # x comes as B, T; token embeddings; B,T,C\n",
    "        tok_emb = self.tok_embeddings(idx)\n",
    "        # creates 1D-tensor with values from 0 - context_len; T\n",
    "        pos_idx = torch.arange(0, T, device=device)\n",
    "        # position embeddings; T, C\n",
    "        pos_emb = self.pos_embeddings(pos_idx)\n",
    "        # combined emds for token + pos; B, T, C\n",
    "        emb = tok_emb + pos_emb\n",
    "        # hidden layers & logits\n",
    "        h = self.t_blocks(emb)\n",
    "        logits = self.lm_head(h)\n",
    "        # calc loss if targets are available; otherwise set loss to None for sampling\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            # flatten logits into B*T, C\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten targets into B*T\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    # generate names of tbd amount; name ends at first line break char\n",
    "    def generate(self, amount_names):\n",
    "        out = []\n",
    "        for _ in range(amount_names):\n",
    "            name = []\n",
    "            # start always with 0 context for linebreak as first char; forward pass expects shape of (1, 1) to work\n",
    "            context = torch.zeros((1, 1), dtype=torch.long)\n",
    "            context = context.to(device)\n",
    "            while True:\n",
    "                # context must not be greater than context_len, otherwise mat mul in forward pass does not work; cut max latest context\n",
    "                context_cut = context[:, -context_len:]\n",
    "                logits, _ = self(context_cut)\n",
    "                # grab logits at last timestep\n",
    "                logits = logits[:, -1, :]\n",
    "                logits = F.softmax(logits, dim=-1)\n",
    "                idx = torch.multinomial(logits, num_samples=1, replacement=True).item()\n",
    "                name.append(itos[idx])\n",
    "                # end name gen when first linebreak is sampled\n",
    "                if idx == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    # as long as no linebreak is hit, add last idx to context and sample next char for name\n",
    "                    context = torch.cat((context, torch.tensor([[idx]], dtype=torch.long, device=device)), dim=1)\n",
    "            out.append(\"\".join(name))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6349373\n"
     ]
    }
   ],
   "source": [
    "# init model, port to gpu, init optimizer, print model params\n",
    "model = GPT()\n",
    "m = model.to(device)\n",
    "optimizer = Optim.Adam(m.parameters(), lr=learning_rate)\n",
    "parameters = m.parameters()\n",
    "print(sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after 0 iterations: train_loss 4.603909492492676; eval_loss 4.60637903213501\n",
      "loss after 1000 iterations: train_loss 1.7214868068695068; eval_loss 1.741114854812622\n",
      "loss after 2000 iterations: train_loss 1.587981104850769; eval_loss 1.6238888502120972\n",
      "loss after 3000 iterations: train_loss 1.5157275199890137; eval_loss 1.577518105506897\n",
      "loss after 4000 iterations: train_loss 1.4594924449920654; eval_loss 1.5488260984420776\n",
      "loss after 5000 iterations: train_loss 1.4075320959091187; eval_loss 1.5284029245376587\n",
      "loss after 6000 iterations: train_loss 1.3645418882369995; eval_loss 1.5264160633087158\n",
      "loss after 7000 iterations: train_loss 1.3213069438934326; eval_loss 1.5272397994995117\n"
     ]
    }
   ],
   "source": [
    "# train model over defined train steps\n",
    "def train_model():\n",
    "\n",
    "    for i in range(train_iter):\n",
    "    \n",
    "        # eval loss & print after certain amount of train steps\n",
    "        if i % eval_interval == 0:\n",
    "            losses = check_loss()\n",
    "            print(f\"loss after {i} iterations: train_loss {losses[train_split]}; eval_loss {losses[dev_split]}\")\n",
    "        \n",
    "        # forward pass\n",
    "        Xtr, Ytr = get_batch(train_split)\n",
    "        Xtr, Ytr = Xtr.to(device), Ytr.to(device)\n",
    "        _, loss = m(Xtr, Ytr)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "\n",
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lindgraben\\n',\n",
       " 'Frankendorf\\n',\n",
       " 'Rißtaubieren\\n',\n",
       " 'Felkendorf\\n',\n",
       " 'Oberstaufen\\n',\n",
       " 'Scharloch\\n',\n",
       " 'Stumpfental\\n',\n",
       " 'Schwalbmühle\\n',\n",
       " 'Wiesenhausen\\n',\n",
       " 'Rothenbuch\\n',\n",
       " 'Atter\\n',\n",
       " 'Westenerhof\\n',\n",
       " 'Einzienla\\n',\n",
       " 'Pötzling\\n',\n",
       " 'Gstrieß\\n',\n",
       " 'Hummelsried\\n',\n",
       " 'Reifling\\n',\n",
       " 'Obernöringen\\n',\n",
       " 'Branntännl\\n',\n",
       " 'Krösel\\n',\n",
       " 'Bärenlingen\\n',\n",
       " 'Breitel\\n',\n",
       " 'Siegskofen\\n',\n",
       " 'Mainauf\\n',\n",
       " 'Bihelhaeden\\n',\n",
       " 'Buxach\\n',\n",
       " 'Wineder\\n',\n",
       " 'Thierau\\n',\n",
       " 'Hofdorf\\n',\n",
       " 'Ruhmannsdorf\\n',\n",
       " 'Scheckenbach\\n',\n",
       " 'Hollberg\\n',\n",
       " 'Reicher Spiedler\\n',\n",
       " 'Wartenhofen\\n',\n",
       " 'Krietzendorf\\n',\n",
       " 'Langenprechting\\n',\n",
       " 'Gstorfing\\n',\n",
       " 'Bugendorf\\n',\n",
       " 'Grimmsried\\n',\n",
       " 'Horrheim\\n',\n",
       " 'Nindicherles\\n',\n",
       " 'Ablom\\n',\n",
       " 'Alte Leonitz\\n',\n",
       " 'Unternore Wald\\n',\n",
       " 'Pliener\\n',\n",
       " 'Lippertsgrün\\n',\n",
       " 'Krippenstaller\\n',\n",
       " 'Querenhofen\\n',\n",
       " 'Kötzwieser\\n',\n",
       " 'Vogelschlag\\n']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from model with amount names\n",
    "m.generate(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN 1: 1.2M params; 7,3 min; train_loss 1.5999797582626343; eval_loss 1.6328703165054321\n",
    "context_len = 64\n",
    "n_embd = 128\n",
    "n_head = 4\n",
    "n_layer = 6\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 5000\n",
    "eval_iter = 150\n",
    "eval_interval = 500\n",
    "dropout = 0.2\n",
    "\"loss after 0 iterations: train_loss 4.494983673095703; eval_loss 4.498855113983154\n",
    "loss after 500 iterations: train_loss 2.1576623916625977; eval_loss 2.1638336181640625\n",
    "loss after 1000 iterations: train_loss 1.8927803039550781; eval_loss 1.8983594179153442\n",
    "loss after 1500 iterations: train_loss 1.794490933418274; eval_loss 1.806968331336975\n",
    "loss after 2000 iterations: train_loss 1.7333722114562988; eval_loss 1.7506181001663208\n",
    "loss after 2500 iterations: train_loss 1.6985934972763062; eval_loss 1.7193843126296997\n",
    "loss after 3000 iterations: train_loss 1.6637684106826782; eval_loss 1.6839677095413208\n",
    "loss after 3500 iterations: train_loss 1.6381772756576538; eval_loss 1.6658086776733398\n",
    "loss after 4000 iterations: train_loss 1.6181026697158813; eval_loss 1.6488206386566162\n",
    "loss after 4500 iterations: train_loss 1.5999797582626343; eval_loss 1.6328703165054321\"\n",
    "['Oberkirchen\\n',\n",
    " 'Riedenog\\n',\n",
    " 'Schömatsbach\\n',\n",
    " 'Löhendorf\\n',\n",
    " 'Zauchberg\\n',\n",
    " 'Hahnath\\n',\n",
    " 'Haid\\n',\n",
    " 'Hintsterheim\\n',\n",
    " 'Kreuzwinden\\n',\n",
    " 'Degrüblach\\n']\n",
    "\n",
    "\n",
    "---\n",
    "# RUN 2: 6.34M params; canceled after 31 min due to overfitting\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 20000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "\n",
    "loss after 0 iterations: train_loss 4.703804969787598; eval_loss 4.707516193389893\n",
    "loss after 1000 iterations: train_loss 1.7233854532241821; eval_loss 1.7396659851074219\n",
    "loss after 2000 iterations: train_loss 1.5916658639907837; eval_loss 1.6276907920837402\n",
    "loss after 3000 iterations: train_loss 1.5190118551254272; eval_loss 1.5804953575134277\n",
    "loss after 4000 iterations: train_loss 1.4579596519470215; eval_loss 1.5506863594055176\n",
    "loss after 5000 iterations: train_loss 1.4088348150253296; eval_loss 1.540738582611084\n",
    "loss after 6000 iterations: train_loss 1.359342336654663; eval_loss 1.5292009115219116\n",
    "loss after 7000 iterations: train_loss 1.3172597885131836; eval_loss 1.527799129486084\n",
    "loss after 8000 iterations: train_loss 1.2786493301391602; eval_loss 1.5266221761703491\n",
    "loss after 9000 iterations: train_loss 1.2427423000335693; eval_loss 1.546021580696106\n",
    "loss after 10000 iterations: train_loss 1.2063122987747192; eval_loss 1.563255786895752\n",
    "\n",
    "---\n",
    "# RUN 3: 14.2M params; canceled after 33 min due to overfitting\n",
    "context_len = 64\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 8\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 10000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "\n",
    "loss after 0 iterations: train_loss 4.475872039794922; eval_loss 4.478583335876465\n",
    "loss after 1000 iterations: train_loss 1.6488356590270996; eval_loss 1.6782140731811523\n",
    "loss after 2000 iterations: train_loss 1.5181366205215454; eval_loss 1.580893874168396\n",
    "loss after 3000 iterations: train_loss 1.4300034046173096; eval_loss 1.5446563959121704\n",
    "loss after 4000 iterations: train_loss 1.358554482460022; eval_loss 1.53664231300354\n",
    "loss after 5000 iterations: train_loss 1.2867943048477173; eval_loss 1.5338422060012817\n",
    "loss after 6000 iterations: train_loss 1.2154250144958496; eval_loss 1.5534075498580933\n",
    "loss after 7000 iterations: train_loss 1.1497846841812134; eval_loss 1.5951995849609375\n",
    "\n",
    "---\n",
    "# RUN 4: 2.7m; canceled after 10 min due to learning rate stalling\n",
    "context_len = 32\n",
    "n_embd = 192\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "train_iter = 8000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.3\n",
    "weight_decay = 0.01\n",
    "\n",
    "loss after 0 iterations: train_loss 4.531078338623047; eval_loss 4.526917457580566\n",
    "loss after 1000 iterations: train_loss 2.3952038288116455; eval_loss 2.3950064182281494\n",
    "loss after 2000 iterations: train_loss 2.3764455318450928; eval_loss 2.3757312297821045\n",
    "loss after 3000 iterations: train_loss 2.367504596710205; eval_loss 2.3657174110412598\n",
    "loss after 4000 iterations: train_loss 2.365919351577759; eval_loss 2.3662781715393066\n",
    "loss after 5000 iterations: train_loss 2.369229555130005; eval_loss 2.365706205368042\n",
    "\n",
    "# RUN 5: 4.7m; canceled after 10 min due to learning rate stalling\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 10000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "weight_decay = 0.01\n",
    "\n",
    "loss after 0 iterations: train_loss 4.481040954589844; eval_loss 4.479975700378418\n",
    "loss after 1000 iterations: train_loss 2.363853693008423; eval_loss 2.3665316104888916\n",
    "loss after 2000 iterations: train_loss 2.3708105087280273; eval_loss 2.3742127418518066\n",
    "loss after 3000 iterations: train_loss 2.3888113498687744; eval_loss 2.3926215171813965\n",
    "\n",
    "# RUN 6: 6.34M params; canceled after 8 min due to significantly worse performance than Run 2\n",
    "-> same as Run 2 but with small L2 weight decay\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 10000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "weight_decay = 0.001\n",
    "\n",
    "loss after 0 iterations: train_loss 4.602679252624512; eval_loss 4.598903656005859\n",
    "loss after 1000 iterations: train_loss 2.1808369159698486; eval_loss 2.190643787384033\n",
    "loss after 2000 iterations: train_loss 2.142751455307007; eval_loss 2.1480987071990967\n",
    "\n",
    "# RUN 7: 6.34M params; 23 min -> 1.52 nlll\n",
    "\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4\n",
    "train_iter = 8000\n",
    "eval_iter = 150\n",
    "eval_interval = 1000\n",
    "dropout = 0.2\n",
    "\n",
    "loss after 0 iterations: train_loss 4.603909492492676; eval_loss 4.60637903213501\n",
    "loss after 1000 iterations: train_loss 1.7214868068695068; eval_loss 1.741114854812622\n",
    "loss after 2000 iterations: train_loss 1.587981104850769; eval_loss 1.6238888502120972\n",
    "loss after 3000 iterations: train_loss 1.5157275199890137; eval_loss 1.577518105506897\n",
    "loss after 4000 iterations: train_loss 1.4594924449920654; eval_loss 1.5488260984420776\n",
    "loss after 5000 iterations: train_loss 1.4075320959091187; eval_loss 1.5284029245376587\n",
    "loss after 6000 iterations: train_loss 1.3645418882369995; eval_loss 1.5264160633087158\n",
    "loss after 7000 iterations: train_loss 1.3213069438934326; eval_loss 1.5272397994995117\n",
    "\n",
    "'Lindgraben\\n',\n",
    " 'Frankendorf\\n',\n",
    " 'Rißtaubieren\\n',\n",
    " 'Felkendorf\\n',\n",
    " 'Oberstaufen\\n',\n",
    " 'Scharloch\\n',\n",
    " 'Stumpfental\\n',\n",
    " 'Schwalbmühle\\n',\n",
    " 'Wiesenhausen\\n',\n",
    " 'Rothenbuch\\n',\n",
    " 'Atter\\n',\n",
    " 'Westenerhof\\n',\n",
    " 'Einzienla\\n',\n",
    " 'Pötzling\\n',\n",
    " 'Gstrieß\\n',\n",
    " 'Hummelsried\\n',\n",
    " 'Reifling\\n',\n",
    " 'Obernöringen\\n',\n",
    " 'Branntännl\\n',\n",
    " 'Krösel\\n',\n",
    " 'Bärenlingen\\n',\n",
    " 'Breitel\\n',\n",
    " 'Siegskofen\\n',\n",
    " 'Mainauf\\n',\n",
    " 'Bihelhaeden\\n',\n",
    " 'Buxach\\n',\n",
    " 'Wineder\\n',\n",
    " 'Thierau\\n',\n",
    " 'Hofdorf\\n',\n",
    " 'Ruhmannsdorf\\n',\n",
    " 'Scheckenbach\\n',\n",
    " 'Hollberg\\n',\n",
    " 'Reicher Spiedler\\n',\n",
    " 'Wartenhofen\\n',\n",
    " 'Krietzendorf\\n',\n",
    " 'Langenprechting\\n',\n",
    " 'Gstorfing\\n',\n",
    " 'Bugendorf\\n',\n",
    " 'Grimmsried\\n',\n",
    " 'Horrheim\\n',\n",
    " 'Nindicherles\\n',\n",
    " 'Ablom\\n',\n",
    " 'Alte Leonitz\\n',\n",
    " 'Unternore Wald\\n',\n",
    " 'Pliener\\n',\n",
    " 'Lippertsgrün\\n',\n",
    " 'Krippenstaller\\n',\n",
    " 'Querenhofen\\n',\n",
    " 'Kötzwieser\\n',\n",
    " 'Vogelschlag\\n']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
