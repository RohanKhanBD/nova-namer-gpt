{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as Optim\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62262\n",
      "['Rieder Berg\\n', 'Alttiefenweg\\n', 'Goßmannsdorf\\n', 'Gemeindebühl\\n', 'Mader\\n', 'Kroissenhof\\n', 'Schlappenreuth\\n', 'Obermitterdorf\\n', 'Ullading\\n', 'Großköllnbach\\n']\n",
      "10.843371558896276\n"
     ]
    }
   ],
   "source": [
    "# import text & shuffle set\n",
    "with open(\"./assets/names.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    names = file.readlines()\n",
    "# shuffle it\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "# dataset size\n",
    "print(len(names))\n",
    "print(names[:10])\n",
    "\n",
    "# avg word len\n",
    "avg_len_words = 0\n",
    "for word in names:\n",
    "    avg_len_words += len(word)\n",
    "print(avg_len_words/len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "['\\n', ' ', '-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ä', 'Ö', 'Ü', 'ß', 'ä', 'ö', 'ü']\n"
     ]
    }
   ],
   "source": [
    "# setup vocabulary\n",
    "# decision to go with a \"1. long streaming approach with multiple names within context\" vs \"2. one name within context padded to fixed len & special start and end chars\"\n",
    "# i adressed my concerns, that it makes no senses in 1. that via transformer tech some name learns pattern from PREVIOUS names to predict next char\n",
    "# o1 recommends approach 1 vs. claude recommends approach 2; both unanimous that both ways will net roughly same perplexity, discussion is about top 5%\n",
    "# maybe i test approach 2 later as comparison\n",
    "all_chars = list(sorted(set([(\"\".join(char)) for name in names for char in name])))\n",
    "print(len(all_chars))\n",
    "print(all_chars)\n",
    "vocab_size = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "context_len = 64\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "batch_size = 128\n",
    "learning_rate = 3e-4\n",
    "train_iter = 1000\n",
    "eval_iter = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: ' ', 2: '-', 3: 'A', 4: 'B', 5: 'C', 6: 'D', 7: 'E', 8: 'F', 9: 'G', 10: 'H', 11: 'I', 12: 'J', 13: 'K', 14: 'L', 15: 'M', 16: 'N', 17: 'O', 18: 'P', 19: 'Q', 20: 'R', 21: 'S', 22: 'T', 23: 'U', 24: 'V', 25: 'W', 26: 'X', 27: 'Z', 28: 'a', 29: 'b', 30: 'c', 31: 'd', 32: 'e', 33: 'f', 34: 'g', 35: 'h', 36: 'i', 37: 'j', 38: 'k', 39: 'l', 40: 'm', 41: 'n', 42: 'o', 43: 'p', 44: 'q', 45: 'r', 46: 's', 47: 't', 48: 'u', 49: 'v', 50: 'w', 51: 'x', 52: 'y', 53: 'z', 54: 'Ä', 55: 'Ö', 56: 'Ü', 57: 'ß', 58: 'ä', 59: 'ö', 60: 'ü'}\n",
      "{'\\n': 0, ' ': 1, '-': 2, 'A': 3, 'B': 4, 'C': 5, 'D': 6, 'E': 7, 'F': 8, 'G': 9, 'H': 10, 'I': 11, 'J': 12, 'K': 13, 'L': 14, 'M': 15, 'N': 16, 'O': 17, 'P': 18, 'Q': 19, 'R': 20, 'S': 21, 'T': 22, 'U': 23, 'V': 24, 'W': 25, 'X': 26, 'Z': 27, 'a': 28, 'b': 29, 'c': 30, 'd': 31, 'e': 32, 'f': 33, 'g': 34, 'h': 35, 'i': 36, 'j': 37, 'k': 38, 'l': 39, 'm': 40, 'n': 41, 'o': 42, 'p': 43, 'q': 44, 'r': 45, 's': 46, 't': 47, 'u': 48, 'v': 49, 'w': 50, 'x': 51, 'y': 52, 'z': 53, 'Ä': 54, 'Ö': 55, 'Ü': 56, 'ß': 57, 'ä': 58, 'ö': 59, 'ü': 60}\n",
      "[20, 36, 32, 31, 32, 45, 1, 4, 32, 45, 34, 0]\n",
      "Rieder Berg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# vocabulary mapping dicts\n",
    "itos = {i:s for i, s in enumerate(all_chars)}\n",
    "stoi = {s:i for i, s in itos.items()}\n",
    "print(itos)\n",
    "print(stoi)\n",
    "# voc encoding / decoding functions\n",
    "encode = lambda input: [stoi[i] for i in input]\n",
    "decode = lambda input: \"\".join([itos[i] for i in input])\n",
    "print(encode(names[0]))\n",
    "print(decode(encode(names[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540104 67513 67513\n"
     ]
    }
   ],
   "source": [
    "# convert names list to data: concat text, encode it, tensor it\n",
    "data = torch.tensor(encode(\"\".join(names)))\n",
    "\n",
    "# split data into train / dev / test with 0.8 / 0.1 / 0.1\n",
    "border_1 = int(0.8 * len(data))\n",
    "border_2 = int(0.9 * len(data))\n",
    "train_split = data[:border_1]\n",
    "dev_split = data[border_1:border_2]\n",
    "test_split = data[border_2:]\n",
    "print(len(train_split), len(dev_split), len(test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64]) torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "# deliver batches of X, Y tensors for chosen split\n",
    "torch.manual_seed(42)\n",
    "def get_batch(split):\n",
    "    \"\"\" delivers a batch of X, Y tensors for specified split\"\"\"\n",
    "    # get random numbers (in amount of \"batch_size\") within split boundaries to grab data for the batch samples\n",
    "    batch_borders = torch.randint(0, len(split)-context_len, (batch_size,))\n",
    "    x = torch.stack([split[t : t+context_len] for t in batch_borders])\n",
    "    y = torch.stack([split[t+1 : t+context_len+1] for t in batch_borders])\n",
    "    return x, y\n",
    "    \n",
    "\n",
    "x, y = get_batch(train_split)\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ffw(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp1 = nn.Linear(n_embd, n_embd, bias=None)\n",
    "        self.mlp2 = nn.Linear(n_embd, n_embd, bias=None)\n",
    "        self.\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# NN classes\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # input layer\n",
    "        self.tok_embeddings = nn.Embedding(vocab_size, n_embd)\n",
    "        self.pos_embeddings = nn.Embedding(context_len, n_embd)\n",
    "        # ffw layer\n",
    "        self.ffw1 = nn.Linear(n_embd, n_embd)\n",
    "        # output layer\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    " \n",
    "    def forward(self, x, targets=None):\n",
    "        \n",
    "        # input layer\n",
    "        # token embeddings; B,T,C\n",
    "        tok_emb = self.tok_embeddings(x)\n",
    "        #print(tok_emb.shape)\n",
    "        # creates 1D-tensor with values from 0 - context_len; T\n",
    "        pos_raw = torch.arange(0, context_len)\n",
    "        # position embeddings; T, C\n",
    "        pos_emb = self.pos_embeddings(pos_raw)\n",
    "        #print(pos_emb.shape)\n",
    "        # combined emds for token + pos; B, T, C\n",
    "        emb = tok_emb + pos_emb\n",
    "\n",
    "        # hidden layers\n",
    "        h = self.ffw1(emb)\n",
    "\n",
    "        logits = self.lm_head(h)\n",
    "\n",
    "        # calc loss if targets are available\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # flatten logits into B*T, C\n",
    "            logits = logits.view(B*T, C)\n",
    "            # flatten targets into B*T\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 64]) torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "m = GPT()\n",
    "x, y = get_batch(train_split)\n",
    "print(x.shape, y.shape)\n",
    "logits, loss = m(x, y)\n",
    "logits.shape\n",
    "optimizer = Optim.Adam(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.1900, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "def train_model():\n",
    "\n",
    "    for i in range(train_iter):\n",
    "    \n",
    "        # forward pass\n",
    "        x, y = get_batch(train_split)\n",
    "        _, loss = m(x, y)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # update params\n",
    "        optimizer.step()\n",
    "\n",
    "        print(loss)\n",
    "        break\n",
    "\n",
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
