{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as D\n",
    "import torch.optim as Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62262\n",
      "['Aach im Allgäu\\n', 'Aach im Allgäu\\n', 'Aach im Allgäu\\n', 'Aacherwiese\\n', 'Aalbuch\\n', 'Aalen\\n', 'Aalholz\\n', 'Aalkorb\\n', 'Abach\\n', 'Abbachhof\\n']\n",
      "10.843371558896276\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "with open(\"./assets/names.txt\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    data = file.readlines()\n",
    "# dataset size\n",
    "print(len(data))\n",
    "print(data[:10])\n",
    "# avg word len\n",
    "avg_len_words = 0\n",
    "for word in data:\n",
    "    avg_len_words += len(word)\n",
    "print(avg_len_words/len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "['\\n', ' ', '-', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ä', 'Ö', 'Ü', 'ß', 'ä', 'ö', 'ü']\n"
     ]
    }
   ],
   "source": [
    "# setup vocabulary\n",
    "# decision to go with a \"1. long streaming approach with multiple names within context\" vs \"2. one name within context padded to fixed len & special start and end chars\"\n",
    "# i adressed my concerns, that it makes no senses in 1. that via transformer tech some name learns pattern from PREVIOUS names to predict next char\n",
    "# o1 recommends approach 1 vs. claude recommends approach 2; both unanimous that both ways will net roughly same perplexity, discussion is about top 5%\n",
    "# maybe i test approach 2 later as comparison\n",
    "all_chars = list(sorted(set([(\"\".join(char)) for word in data for char in word])))\n",
    "print(len(all_chars))\n",
    "print(all_chars)\n",
    "vocab_size = len(all_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "context_len = 64\n",
    "n_embd = 256\n",
    "n_head = 8\n",
    "n_layer = 6\n",
    "batch_size = 64\n",
    "learning_rate = 3e-4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
